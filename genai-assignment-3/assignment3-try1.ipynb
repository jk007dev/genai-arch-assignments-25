{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Agentic RAG System with Gemini, LangGraph, and MLflow\n",
    "\n",
    "Assignment 3 \n",
    "\n",
    "### Core Technologies that will be targeted:\n",
    "\n",
    "- **LLM & Embeddings**: Gemini 1.5 Pro on Google Cloud Vertex AI\n",
    "- **Agentic Flows**: LangGraph\n",
    "- **Vector Database**: Pinecone\n",
    "- **Experiment Tracking**: MLflow\n",
    "- **Language**: Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we need to install all the required Python packages. If you are running this in a new environment, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph google-cloud-aiplatform pinecone-client pydantic mlflow python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup and Configuration\n",
    "\n",
    "### A. Import Libraries\n",
    "We'll import all the necessary libraries for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from typing import List, TypedDict, Optional\n",
    "from getpass import getpass\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Vertex AI (Gemini)\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "# Pinecone\n",
    "import pinecone\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "\n",
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Configure Logging\n",
    "A proper logging setup helps in debugging and monitoring the flow of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP and Vertex AI Initialization\n",
    "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "if not GCP_PROJECT_ID:\n",
    "    GCP_PROJECT_ID = input(\"Please enter your GCP Project ID: \")\n",
    "vertexai.init(project=GCP_PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "# Pinecone Initialization\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    PINECONE_API_KEY = getpass(\"Please enter your Pinecone API Key: \")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "if not PINECONE_ENVIRONMENT:\n",
    "    PINECONE_ENVIRONMENT = input(\"Please enter your Pinecone Environment (e.g., 'gcp-starter'): \")\n",
    "    \n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "\n",
    "logging.info(\"Successfully connected to Vertex AI and Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocessing & Indexing\n",
    "\n",
    "In this step, we'll load our knowledge base, generate embeddings for each entry, and store them in a Pinecone vector index.\n",
    "\n",
    "### A. Prepare the Dataset\n",
    "For this demo, we'll create the `dataset.json` file directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_data = [\n",
    "    {\n",
    "        \"doc_id\": \"KB001\",\n",
    "        \"question\": \"What are best practices for debugging Python?\",\n",
    "        \"answer_snippet\": \"When debugging Python, standard best practices include using the built-in PDB (Python Debugger), understanding tracebacks thoroughly, and strategically placing print statements to trace variable states. PDB allows you to step through code line by line.\",\n",
    "        \"source\": \"python_debugging_guide.md\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"KB002\",\n",
    "        \"question\": \"How do I tune performance in a Python application?\",\n",
    "        \"answer_snippet\": \"Performance tuning in Python starts with profiling. Use tools like cProfile to identify bottlenecks. Common optimization techniques include using efficient data structures, avoiding global variables, and leveraging memoization for expensive function calls.\",\n",
    "        \"source\": \"performance_tuning.md\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"KB003\",\n",
    "        \"question\": \"What are Python virtual environments?\",\n",
    "        \"answer_snippet\": \"A Python virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. It helps manage dependencies and avoid conflicts between projects.\",\n",
    "        \"source\": \"virtual_environments.md\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"KB004\",\n",
    "        \"question\": \"What are some less common debugging techniques in Python?\",\n",
    "        \"answer_snippet\": \"Advanced and less common debugging techniques include using the logging module for robust, configurable output, and 'rubber duck debugging' where you explain your code line-by-line to an inanimate object to spot logical errors.\",\n",
    "        \"source\": \"advanced_debugging.md\"\n",
    "    }\n",
    "]\n",
    "\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(kb_data, f, indent=2)\n",
    "\n",
    "logging.info(\"'dataset.json' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Define Helper Functions for Embeddings and LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "llm_model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generates embeddings for a given text using Vertex AI.\"\"\"\n",
    "    # Vertex AI has a limit of 5 texts per call\n",
    "    embeddings = embedding_model.get_embeddings([text])\n",
    "    return embeddings[0].values\n",
    "\n",
    "def call_gemini_llm(prompt: str) -> str:\n",
    "    \"\"\"Calls the Gemini LLM with a specific prompt and temperature.\"\"\"\n",
    "    response = llm_model.generate_content(\n",
    "        [Part.from_text(prompt)],\n",
    "        generation_config={\n",
    "            \"temperature\": 0.0, # for consistency\n",
    "            \"max_output_tokens\": 1024\n",
    "        }\n",
    "    )\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Index the Data in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_INDEX_NAME = \"agentic-rag-index\"\n",
    "\n",
    "def preprocess_and_index(json_path: str):\n",
    "    \"\"\"Loads, preprocesses, and indexes the knowledge base in Pinecone.\"\"\"\n",
    "    logging.info(\"--- Starting Preprocessing and Indexing ---\")\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
    "        # The embedding dimension for textembedding-gecko is 768\n",
    "        pinecone.create_index(name=PINECONE_INDEX_NAME, dimension=768, metric='cosine')\n",
    "        logging.info(f\"Created Pinecone index: {PINECONE_INDEX_NAME}\")\n",
    "    \n",
    "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        knowledge_base = json.load(f)\n",
    "    \n",
    "    logging.info(f\"Processing {len(knowledge_base)} documents...\")\n",
    "    vectors_to_upsert = []\n",
    "    for item in knowledge_base:\n",
    "        embedding = get_embedding(item['answer_snippet'])\n",
    "        vector = {\n",
    "            'id': item['doc_id'],\n",
    "            'values': embedding,\n",
    "            'metadata': {\n",
    "                'text': item['answer_snippet'],\n",
    "                'source': item['source']\n",
    "             }\n",
    "        }\n",
    "        vectors_to_upsert.append(vector)\n",
    "        \n",
    "    # Upsert in batches (Pinecone recommends batches of 100 or less)\n",
    "    if vectors_to_upsert:\n",
    "        index.upsert(vectors=vectors_to_upsert)\n",
    "        logging.info(f\"Successfully upserted {len(vectors_to_upsert)} vectors into Pinecone.\")\n",
    "    \n",
    "    logging.info(\"--- Preprocessing and Indexing Complete ---\")\n",
    "    return index\n",
    "\n",
    "# Run the indexing process\n",
    "pinecone_index = preprocess_and_index(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the LangGraph Workflow\n",
    "\n",
    "Now we'll define the state, nodes, and edges for our agentic graph.\n",
    "\n",
    "### A. Define the Graph State\n",
    "The state is a dictionary that gets passed between nodes, carrying all the necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        snippets: A list of retrieved document snippets.\n",
    "        answer: The LLM-generated answer.\n",
    "        critique: The critique of the generated answer.\n",
    "        refine_count: The number of refinement loops.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    snippets: List[dict]\n",
    "    answer: str\n",
    "    critique: str\n",
    "    refine_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Define the Graph Nodes\n",
    "Each node is a function that performs a specific action (retrieve, generate, critique, refine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_kb(state: GraphState) -> GraphState:\n",
    "    \"\"\"Retrieves relevant snippets from the knowledge base.\"\"\"\n",
    "    logging.info(\"--- Node: retrieve_kb ---\")\n",
    "    question = state[\"question\"]\n",
    "    query_embedding = get_embedding(question)\n",
    "    \n",
    "    # Retrieve top 2 snippets initially to encourage refinement loop\n",
    "    retrieved = pinecone_index.query(query_embedding, top_k=2, include_metadata=True)\n",
    "    \n",
    "    snippets = []\n",
    "    for match in retrieved['matches']:\n",
    "        snippets.append({\n",
    "            'doc_id': match['id'], \n",
    "            'answer_snippet': match['metadata']['text'],\n",
    "            'source': match['metadata']['source']\n",
    "        })\n",
    "    \n",
    "    state[\"snippets\"] = snippets\n",
    "    mlflow.log_text(json.dumps(snippets, indent=2), \"1_initial_snippets.json\")\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Generates an answer using the retrieved snippets.\"\"\"\n",
    "    logging.info(\"--- Node: generate_answer ---\")\n",
    "    question = state[\"question\"]\n",
    "    snippets = state[\"snippets\"]\n",
    "    \n",
    "    context = \"\\n\".join([f\"{s['doc_id']}: {s['answer_snippet']}\" for s in snippets])\n",
    "    prompt = f\"\"\"Context snippets:\n",
    "{context}\n",
    "\n",
    "Generate a comprehensive answer for the question below, citing the sources using their doc_id (e.g., [KB001]).\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    \n",
    "    answer = call_gemini_llm(prompt)\n",
    "    logging.info(f\"Generated Answer: {answer}\")\n",
    "    state[\"answer\"] = answer\n",
    "    mlflow.log_text(answer, f\"{2 if state['refine_count'] == 0 else 5}_generated_answer.txt\")\n",
    "    return state\n",
    "\n",
    "def critique_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Critiques the generated answer for completeness.\"\"\"\n",
    "    logging.info(\"--- Node: critique_answer ---\")\n",
    "    question = state[\"question\"]\n",
    "    snippets = state[\"snippets\"]\n",
    "    answer = state[\"answer\"]\n",
    "\n",
    "    context = \"\\n\".join([f\"{s['doc_id']}: {s['answer_snippet']}\" for s in snippets])\n",
    "    prompt = f\"\"\"You are a meticulous quality checker.\n",
    "Question: {question}\n",
    "Context Snippets:\n",
    "{context}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Does the 'Answer' fully address the 'Question' based *only* on the 'Context Snippets'?\n",
    "If it is complete, respond with only the word \"COMPLETE\".\n",
    "If it is not complete, respond with \"REFINE:\" followed by a concise list of missing keywords or concepts.\n",
    "\"\"\"\n",
    "    critique = call_gemini_llm(prompt)\n",
    "    logging.info(f\"Critique: {critique}\")\n",
    "    state[\"critique\"] = critique\n",
    "    mlflow.log_text(critique, \"3_critique.txt\")\n",
    "    return state\n",
    "\n",
    "def refine_answer(state: GraphState) -> GraphState:\n",
    "    \"\"\"Refines the answer by retrieving more context and regenerating.\"\"\"\n",
    "    logging.info(\"--- Node: refine_answer ---\")\n",
    "    question = state[\"question\"]\n",
    "    critique = state[\"critique\"]\n",
    "    \n",
    "    missing_info = critique.replace(\"REFINE:\", \"\").strip()\n",
    "    refinement_query = f\"{question} {missing_info}\"\n",
    "    query_embedding = get_embedding(refinement_query)\n",
    "    \n",
    "    # Retrieve one more relevant snippet\n",
    "    retrieved = pinecone_index.query(query_embedding, top_k=1, include_metadata=True)\n",
    "    new_snippet = []\n",
    "    if retrieved['matches']:\n",
    "        match = retrieved['matches'][0]\n",
    "        new_snippet.append({\n",
    "            'doc_id': match['id'], \n",
    "            'answer_snippet': match['metadata']['text'],\n",
    "            'source': match['metadata']['source']\n",
    "        })\n",
    "    \n",
    "    updated_snippets = state[\"snippets\"] + new_snippet\n",
    "    state[\"snippets\"] = updated_snippets\n",
    "    mlflow.log_text(json.dumps(updated_snippets, indent=2), \"4_refined_snippets.json\")\n",
    "    \n",
    "    # Regenerate the answer with the richer context\n",
    "    logging.info(\"Regenerating answer with additional context...\")\n",
    "    return generate_answer(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define Conditional Edges\n",
    "This logic decides which node to go to next based on the critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_refine(state: GraphState) -> str:\n",
    "    \"\"\"Determines whether to refine the answer or end the process.\"\"\"\n",
    "    logging.info(\"--- Decision: should_refine ---\")\n",
    "    critique = state[\"critique\"]\n",
    "    # Limit refinement to one loop to avoid cycles\n",
    "    if state[\"refine_count\"] > 0:\n",
    "        logging.info(\"Decision: Max refinements reached. END\")\n",
    "        return \"end\"\n",
    "        \n",
    "    if critique.startswith(\"REFINE\"):\n",
    "        logging.info(\"Decision: REFINE answer.\")\n",
    "        state[\"refine_count\"] += 1\n",
    "        return \"refine_answer\"\n",
    "    else:\n",
    "        logging.info(\"Decision: Answer is COMPLETE. END\")\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Assemble the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve_kb\", retrieve_kb)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"critique_answer\", critique_answer)\n",
    "workflow.add_node(\"refine_answer\", refine_answer)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"retrieve_kb\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"retrieve_kb\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", \"critique_answer\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique_answer\",\n",
    "    should_refine,\n",
    "    {\n",
    "        \"refine_answer\": \"refine_answer\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"refine_answer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agentic_rag_app = workflow.compile()\n",
    "\n",
    "logging.info(\"LangGraph workflow compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute the Workflow\n",
    "\n",
    "Finally, we run the agent with a sample question. We will wrap the execution inside an MLflow run to log all the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Agentic RAG Run\") as run:\n",
    "    logging.info(f\"\\n{'='*50}\")\n",
    "    logging.info(f\" Kicking off Agentic RAG Workflow (MLflow Run ID: {run.info.run_id}) \".center(50, '='))\n",
    "    logging.info(f\"{'='*50}\\n\")\n",
    "\n",
    "    # This question is designed to miss the 'advanced' part on the first pass\n",
    "    user_question = \"What are best practices for debugging Python, including less common techniques?\"\n",
    "    mlflow.log_param(\"user_question\", user_question)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"question\": user_question,\n",
    "        \"refine_count\": 0\n",
    "    }\n",
    "    \n",
    "    final_state = agentic_rag_app.invoke(initial_state)\n",
    "    \n",
    "    logging.info(f\"\\n{'='*50}\")\n",
    "    logging.info(\" Workflow Complete \".center(50, '='))\n",
    "    logging.info(f\"{'='*50}\\n\")\n",
    "    print(f\"Initial Question: {final_state['question']}\")\n",
    "    print(f\"\\nFinal Answer:\\n{final_state['answer']}\")\n",
    "    print(\"\\nCited Sources:\")\n",
    "    for snippet in final_state['snippets']:\n",
    "        print(f\"- [{snippet['doc_id']}] {snippet['source']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
